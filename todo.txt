- nettoyage des fichiers
notes/corrupter.xml
c.xml
perl.xml

- ajouter vérification de la langue
ne pas vérifier les fichiers PDF

- ajouter une règle : la date de publication doit être après la date de création

- ajouter une règle : la date de publication doit être après la date de création

- ajouter notion de date de publication
gérer la date de publication coté client

- corriger le fait que les liens soient chargés deux fois

- si un lien n'est plus zombie, on ne le détecte pas car on ne vérifie pas les données des liens zombie

- LinkData.getStatus() devrait être un enum
  LinkData.getProtection() devrait être un enum

- ne plus utiliser Google Analytics mais les stat d'Infomaniac

- voir comment purger le cache après les mises à jour

- la sauvegarde du rapport de liens toutes les 30 secondes ne marche plus

- Chrome indique qu'il n'arrive pas à charger les sourcemaps

- supprimer la directory attic
de plus cela génère des erreurs
                   Thread-3 | 20200416T073001.802 | ERROR | URL ../attic/optimtut1.ps.gz is not checked because the URL is malformed
                   Thread-3 | 20200416T073001.993 | ERROR | URL ../attic/tgr_softwareremodeling.pdf is not checked because the URL is malformed

- résoudre la vidéo morte de data.xml (i.e. ne pas vérifier les vidéos zombie)

- ajouter une règle comme quoi il n'y a pas de <ST> pour youtube

- private boolean isDataExpected() { //TBD this method is very stupid, we should used a flag instead of computing the status every time

- corriger problème de certificat

- bug dans HTMLGenerator.java, si le fichier XSLT est modifié, la nouvelle version n'est pas prise en compte

- LinkedIn renvoie un HTTP code 999

- fix incorrect status of running link checker

- détecter les URL qui finissent par deux slashes

- problème avec le lien http://marie.desplats.free.fr/diagora_home_page.html

- bug : s'il n'y a aucun lien dans une page, le texte du rapport reste "analyse en cours"  
  
- Tester robot.txt avec https://www.google.com/webmasters/tools/robots-testing-tool

- Ajouter la détection des malwares

- Ajouter la règle sur les URLs qui apparaissent plusieurs fois

- Accelérer la création des fichier JSON (en extrayant les données de chaque fichier à la modification de celui-ci, en mettant cela dans un fichier de résumé et en ayant la création des JSON qui ne fasse que consolider ces fichiers) 

- ajouter la vérification des <FEED>s

- ajouter support des <MIDDLENAME> multiples

- mettre en place le système des keywords
